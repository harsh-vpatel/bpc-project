Starting preprocessing for hsb-de...
Step 1: Normalizing punctuation...
Step 2: Tokenizing...
Step 3: Cleaning corpus...
Step 4: Training truecaser...
Step 5: Applying truecasing...
Creating final Moses parallel corpus...
Step 6: Splitting into train/dev/test sets...
Total lines:   367535, Train: 294028, Dev: 36753
Step 7: Learning BPE on training data...
Combined training data has   588056 lines
BPE codes learned successfully (   16001 operations)
Step 8: Applying BPE to all splits...
Step 9: Creating fairseq binary dataset...
Fairseq binary dataset created successfully!
=== Preprocessing Statistics ===
Original sentences:   367536
After cleaning:   367535
Final sentences:   367535

=== Split Statistics ===
Training sentences:   294028
Development sentences:    36753
Test sentences:    36754
BPE operations: 16000

Preprocessing complete!

=== Directory Structure ===
./dataset/original/ - Original input files
./dataset/output_moses/ - Moses preprocessing output, train/dev/test splits
./dataset/output_bpe/ - BPE codes and tokenized files
./dataset/fairseq/ - Fairseq binary dataset (ready for training)

=== Key Files for NMT Training ===
Fairseq binary data: ./dataset/fairseq/
BPE codes: ./dataset/output_bpe/bpe.codes
Truecaser models: ./dataset/output_moses/truecase-model.hsb, ./dataset/output_moses/truecase-model.de

=== Training Command ===
CUDA_VISIBLE_DEVICES=0,1 fairseq-train \
    ./dataset/fairseq \
    --arch transformer_iwslt_de_en \
    --share-decoder-input-output-embed \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \
    --dropout 0.3 --weight-decay 0.0001 \
    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \
    --max-tokens 8192 \
    --eval-bleu \
    --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' \
    --eval-bleu-detok moses \
    --eval-bleu-remove-bpe \
    --eval-bleu-print-samples \
    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \
    --save-dir checkpoints/sorbian_german_transformer \
    --ddp-backend=pytorch_ddp \
    --distributed-world-size 2 \
    --distributed-port 12345

Ready for NMT training!
