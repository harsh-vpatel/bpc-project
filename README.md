# BPC Project

## Install

```bash
git clone git@github.com:nilsimda/bpc-project.git && cd bpc-project
uv venv --python 3.10 # seems to work better with fairseq
uv pip install subword-nmt
uv pip install fairseq
source .venv/bin/activate
```

## Prepare dataset

```bash
./prepare.sh

```

Here is how it works:

```mermaid
graph TD
    %% Input Files
    A[ğŸ“ Original Input Files<br/>input.hsb & input.de<br/>in ./dataset/original/] --> B{ğŸ“‹ Validation Check<br/>Files exist?<br/>Dependencies installed?}
    
    B -->|âŒ Missing| C[ğŸ›‘ Exit with Error<br/>- Check input files<br/>- Install subword-nmt<br/>- Install fairseq<br/>- Install bc]
    B -->|âœ… Valid| D[ğŸ§¹ Clean Previous Runs<br/>Remove files from output dirs]
    
    %% Moses Preprocessing Chain
    D --> E[ğŸ“ Step 1: Normalize Punctuation<br/>normalize-punctuation.perl<br/>â†’ corpus.norm.hsb/.de]
    E --> F[ğŸ”¤ Step 2: Tokenization<br/>tokenizer.perl -a<br/>â†’ corpus.tok.hsb/.de]
    F --> G[ğŸ§½ Step 3: Clean Corpus<br/>clean-corpus-n.perl<br/>Remove length 1-100 filter<br/>â†’ corpus.clean.hsb/.de]
    G --> H[ğŸ“ Step 4: Train Truecaser<br/>train-truecaser.perl<br/>â†’ truecase-model.hsb/.de]
    H --> I[ğŸ“ Step 5: Apply Truecasing<br/>truecase.perl<br/>â†’ corpus.tc.hsb/.de]
    
    %% Data Splitting
    I --> J[ğŸ“Š Step 6: Data Splitting<br/>Calculate split sizes:<br/>80% train, 10% dev, 10% test]
    J --> K[âœ‚ï¸ Split Source Files<br/>head/tail commands<br/>â†’ train.hsb, dev.hsb, test.hsb]
    J --> L[âœ‚ï¸ Split Target Files<br/>head/tail commands<br/>â†’ train.de, dev.de, test.de]
    
    %% BPE Processing
    K --> M[ğŸ”— Step 7: Learn BPE<br/>Combine train.hsb + train.de<br/>â†’ train_combined.txt]
    L --> M
    M --> N[ğŸ§  Learn BPE Codes<br/>subword-nmt learn-bpe<br/>16000 operations, min-freq 2<br/>â†’ bpe.codes]
    N --> O{ğŸ“ BPE Validation<br/>Codes file created?<br/>Non-empty?}
    O -->|âŒ Failed| P[ğŸ›‘ BPE Error<br/>Reduce BPE_OPERATIONS<br/>for small datasets]
    O -->|âœ… Success| Q[ğŸ”§ Step 8: Apply BPE<br/>subword-nmt apply-bpe<br/>to all train/dev/test splits]
    
    %% Fairseq Processing
    Q --> R[ğŸ“¦ Step 9: Fairseq Preprocessing<br/>fairseq-preprocess<br/>Create binary dataset]
    R --> S[ğŸ“ˆ Statistics Generation<br/>Count sentences at each stage<br/>Display processing summary]
    
    %% Output Structure
    S --> T[ğŸ“ Final Directory Structure]
    T --> U[ğŸ“‚ ./dataset/original/<br/>â”œâ”€â”€ input.hsb<br/>â””â”€â”€ input.de]
    T --> V[ğŸ“‚ ./dataset/output_moses/<br/>â”œâ”€â”€ Normalized/tokenized files<br/>â”œâ”€â”€ train/dev/test splits<br/>â”œâ”€â”€ truecase models<br/>â””â”€â”€ final_corpus.txt]
    T --> W[ğŸ“‚ ./dataset/output_bpe/<br/>â”œâ”€â”€ bpe.codes<br/>â”œâ”€â”€ train_combined.txt<br/>â””â”€â”€ *.bpe files for all splits]
    T --> X[ğŸ“‚ ./dataset/fairseq/<br/>â”œâ”€â”€ dict.hsb.txt<br/>â”œâ”€â”€ dict.de.txt<br/>â”œâ”€â”€ train.hsb-de.hsb.bin/idx<br/>â”œâ”€â”€ train.hsb-de.de.bin/idx<br/>â”œâ”€â”€ valid.hsb-de.hsb.bin/idx<br/>â”œâ”€â”€ valid.hsb-de.de.bin/idx<br/>â”œâ”€â”€ test.hsb-de.hsb.bin/idx<br/>â””â”€â”€ test.hsb-de.de.bin/idx]
    
    %% Ready for Training
    X --> Y[ğŸš€ Ready for NMT Training<br/>Transformer IWSLT architecture<br/>fairseq-train command<br/>Multi-GPU setup ready]
    
    %% Styling
    classDef inputFile fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef process fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef error fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef output fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef final fill:#fff8e1,stroke:#f57f17,stroke-width:3px
    
    class A,U inputFile
    class E,F,G,H,I,J,K,L,M,N,Q,R,S process
    class B,O decision
    class C,P error
    class V,W,X output
    class Y final
```

```
